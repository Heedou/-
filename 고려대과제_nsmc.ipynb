{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "고려대 과제_nsmc2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heedou/-/blob/main/%EA%B3%A0%EB%A0%A4%EB%8C%80_%EA%B3%BC%EC%A0%9C_nsmc2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XPDKcxSX45I"
      },
      "source": [
        "필요 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhdPrWx_X_J1",
        "outputId": "e94ffe8e-8e6f-44f5-eaa7-63316a06e30b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2AxoyyKeFre",
        "outputId": "7810121c-3864-4d08-a1c8-b14ac71a191e"
      },
      "source": [
        "!pip install SentencePiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2GeI2G3ZItl"
      },
      "source": [
        "필요한 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1V857z_Ya1o"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from transformers import *\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrIiIgyOZNOy"
      },
      "source": [
        "내 구글 드라이브로 연동하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3GNd4xsY5OD",
        "outputId": "067c8198-f836-4c69-f7fa-d9ad58a8e56d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-StMBq0ZxAS"
      },
      "source": [
        "네이버 감성분석 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx0tlFLoZ2up",
        "outputId": "dba4f5b0-d5d6-44bf-d2a4-3850817f9217"
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatwYP6uZ9ap"
      },
      "source": [
        "다운로드 받은 nsmc 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ-FnEV5aBGw",
        "outputId": "371364cf-1048-46db-c172-f552c49a9d2d"
      },
      "source": [
        "os.listdir('nsmc')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['code',\n",
              " '.git',\n",
              " 'ratings.txt',\n",
              " 'README.md',\n",
              " 'ratings_test.txt',\n",
              " 'synopses.json',\n",
              " 'ratings_train.txt',\n",
              " 'raw']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCy8U1yBaEmg"
      },
      "source": [
        "test 데이터와 train 데이터로 나누어 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ean63Gt5ablD"
      },
      "source": [
        "# train 네이버 데이터 불러오기\r\n",
        "train_nsmc = pd.read_table('nsmc/ratings_train.txt')\r\n",
        "train_nsmc['document'] = train_nsmc['document'].str.replace('[^ㄱ-ㅎ가-힣ㅏ-ㅣ\\\\s\\s]', '')\r\n",
        "train_nsmc['document'].replace('', np.nan, inplace=True)\r\n",
        "train_nsmc = train_nsmc.dropna(how='any') #결측치 제거\r\n",
        "train_nsmc = train_nsmc.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tmr-EMIcjXS"
      },
      "source": [
        "# test 네이버 데이터 불러오기\r\n",
        "test_nsmc = pd.read_table('nsmc/ratings_test.txt')\r\n",
        "test_nsmc['document'] = test_nsmc['document'].str.replace('[^ㄱ-ㅎ가-힣ㅏ-ㅣ\\\\s\\s]', '')\r\n",
        "test_nsmc['document'].replace('', np.nan, inplace=True)\r\n",
        "test_nsmc = test_nsmc.dropna(how='any') #결측치 제거\r\n",
        "test_nsmc = test_nsmc.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BX3QOhwa_Nc"
      },
      "source": [
        "데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "tjtQ0R5gbCSC",
        "outputId": "bf79edc8-659b-4512-aa4f-b98e740b9cd9"
      },
      "source": [
        "train_nsmc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3819312</td>\n",
              "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149181</th>\n",
              "      <td>149995</td>\n",
              "      <td>6222902</td>\n",
              "      <td>인간이 문제지 소는 뭔죄인가</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149182</th>\n",
              "      <td>149996</td>\n",
              "      <td>8549745</td>\n",
              "      <td>평점이 너무 낮아서</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149183</th>\n",
              "      <td>149997</td>\n",
              "      <td>9311800</td>\n",
              "      <td>이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149184</th>\n",
              "      <td>149998</td>\n",
              "      <td>2376369</td>\n",
              "      <td>청춘 영화의 최고봉방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149185</th>\n",
              "      <td>149999</td>\n",
              "      <td>9619869</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149186 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  ...  label\n",
              "0            0  ...      0\n",
              "1            1  ...      1\n",
              "2            2  ...      0\n",
              "3            3  ...      0\n",
              "4            4  ...      1\n",
              "...        ...  ...    ...\n",
              "149181  149995  ...      0\n",
              "149182  149996  ...      1\n",
              "149183  149997  ...      0\n",
              "149184  149998  ...      1\n",
              "149185  149999  ...      0\n",
              "\n",
              "[149186 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJINiGk7bI8Q"
      },
      "source": [
        "Kobert 사용한 모델 만들기 - kobert 토크나이저 불러오기\r\n",
        "출처 : https://github.com/monologg/KoBERT-NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cunn5E-qbEMu"
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "from shutil import copyfile\r\n",
        "\r\n",
        "from transformers import PreTrainedTokenizer\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\r\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\r\n",
        "\r\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\r\n",
        "    \"vocab_file\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\r\n",
        "    },\r\n",
        "    \"vocab_txt\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\r\n",
        "    \"monologg/kobert\": 512,\r\n",
        "    \"monologg/kobert-lm\": 512,\r\n",
        "    \"monologg/distilkobert\": 512\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_INIT_CONFIGURATION = {\r\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\r\n",
        "}\r\n",
        "\r\n",
        "SPIECE_UNDERLINE = u'▁'\r\n",
        "\r\n",
        "\r\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\r\n",
        "    \"\"\"\r\n",
        "        SentencePiece based tokenizer. Peculiarities:\r\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\r\n",
        "    \"\"\"\r\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\r\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\r\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\r\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            vocab_file,\r\n",
        "            vocab_txt,\r\n",
        "            do_lower_case=False,\r\n",
        "            remove_space=True,\r\n",
        "            keep_accents=False,\r\n",
        "            unk_token=\"[UNK]\",\r\n",
        "            sep_token=\"[SEP]\",\r\n",
        "            pad_token=\"[PAD]\",\r\n",
        "            cls_token=\"[CLS]\",\r\n",
        "            mask_token=\"[MASK]\",\r\n",
        "            **kwargs):\r\n",
        "        super().__init__(\r\n",
        "            unk_token=unk_token,\r\n",
        "            sep_token=sep_token,\r\n",
        "            pad_token=pad_token,\r\n",
        "            cls_token=cls_token,\r\n",
        "            mask_token=mask_token,\r\n",
        "            **kwargs\r\n",
        "        )\r\n",
        "\r\n",
        "        # Build vocab\r\n",
        "        self.token2idx = dict()\r\n",
        "        self.idx2token = []\r\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\r\n",
        "            for idx, token in enumerate(f):\r\n",
        "                token = token.strip()\r\n",
        "                self.token2idx[token] = idx\r\n",
        "                self.idx2token.append(token)\r\n",
        "\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "\r\n",
        "        self.do_lower_case = do_lower_case\r\n",
        "        self.remove_space = remove_space\r\n",
        "        self.keep_accents = keep_accents\r\n",
        "        self.vocab_file = vocab_file\r\n",
        "        self.vocab_txt = vocab_txt\r\n",
        "\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(vocab_file)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def vocab_size(self):\r\n",
        "        return len(self.idx2token)\r\n",
        "\r\n",
        "    def get_vocab(self):\r\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\r\n",
        "\r\n",
        "    def __getstate__(self):\r\n",
        "        state = self.__dict__.copy()\r\n",
        "        state[\"sp_model\"] = None\r\n",
        "        return state\r\n",
        "\r\n",
        "    def __setstate__(self, d):\r\n",
        "        self.__dict__ = d\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(self.vocab_file)\r\n",
        "\r\n",
        "    def preprocess_text(self, inputs):\r\n",
        "        if self.remove_space:\r\n",
        "            outputs = \" \".join(inputs.strip().split())\r\n",
        "        else:\r\n",
        "            outputs = inputs\r\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\r\n",
        "\r\n",
        "        if not self.keep_accents:\r\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\r\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\r\n",
        "        if self.do_lower_case:\r\n",
        "            outputs = outputs.lower()\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\r\n",
        "        \"\"\" Tokenize a string. \"\"\"\r\n",
        "        text = self.preprocess_text(text)\r\n",
        "\r\n",
        "        if not sample:\r\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\r\n",
        "        else:\r\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\r\n",
        "        new_pieces = []\r\n",
        "        for piece in pieces:\r\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\r\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\r\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\r\n",
        "                    if len(cur_pieces[0]) == 1:\r\n",
        "                        cur_pieces = cur_pieces[1:]\r\n",
        "                    else:\r\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\r\n",
        "                cur_pieces.append(piece[-1])\r\n",
        "                new_pieces.extend(cur_pieces)\r\n",
        "            else:\r\n",
        "                new_pieces.append(piece)\r\n",
        "\r\n",
        "        return new_pieces\r\n",
        "\r\n",
        "    def _convert_token_to_id(self, token):\r\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\r\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\r\n",
        "\r\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\r\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\r\n",
        "        return self.idx2token[index]\r\n",
        "\r\n",
        "    def convert_tokens_to_string(self, tokens):\r\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\r\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\r\n",
        "        return out_string\r\n",
        "\r\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
        "        by concatenating and adding special tokens.\r\n",
        "        A KoBERT sequence has the following format:\r\n",
        "            single sequence: [CLS] X [SEP]\r\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\r\n",
        "        \"\"\"\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\r\n",
        "\r\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\r\n",
        "        \"\"\"\r\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\r\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\r\n",
        "        Args:\r\n",
        "            token_ids_0: list of ids (must not contain special tokens)\r\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\r\n",
        "                for sequence pairs\r\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\r\n",
        "                special tokens for the model\r\n",
        "        Returns:\r\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if already_has_special_tokens:\r\n",
        "            if token_ids_1 is not None:\r\n",
        "                raise ValueError(\r\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\r\n",
        "                    \"ids is already formated with special tokens for the model.\"\r\n",
        "                )\r\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\r\n",
        "\r\n",
        "        if token_ids_1 is not None:\r\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\r\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\r\n",
        "\r\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\r\n",
        "        A KoBERT sequence pair mask has the following format:\r\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\r\n",
        "        | first sequence    | second sequence\r\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\r\n",
        "        \"\"\"\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return len(cls + token_ids_0 + sep) * [0]\r\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\r\n",
        "\r\n",
        "    def save_vocabulary(self, save_directory):\r\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\r\n",
        "            to a directory.\r\n",
        "        \"\"\"\r\n",
        "        if not os.path.isdir(save_directory):\r\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\r\n",
        "            return\r\n",
        "\r\n",
        "        # 1. Save sentencepiece model\r\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\r\n",
        "\r\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\r\n",
        "            copyfile(self.vocab_file, out_vocab_model)\r\n",
        "\r\n",
        "        # 2. Save vocab.txt\r\n",
        "        index = 0\r\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\r\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\r\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\r\n",
        "                if index != token_index:\r\n",
        "                    logger.warning(\r\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\r\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\r\n",
        "                    )\r\n",
        "                    index = token_index\r\n",
        "                writer.write(token + \"\\n\")\r\n",
        "                index += 1\r\n",
        "\r\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQkCzUDHc2QU"
      },
      "source": [
        "토크나이저를 이용한 전체 데이터 인코딩 - 세그먼트, 마스크, 토큰 인풋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb2on8yhdmPK"
      },
      "source": [
        "kobert_tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\r\n",
        "max_sequence_length = 64\r\n",
        "token = kobert_tokenizer.encode('흠포스터', max_length=max_sequence_length, padding='max_length')\r\n",
        "\r\n",
        "for index, value in enumerate(token):\r\n",
        "    if value == 1:\r\n",
        "        token[index] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynq9w4pl5tYM",
        "outputId": "e375f50f-8e96-4492-badd-9e17411bf50b"
      },
      "source": [
        "tokens = []\r\n",
        "masks = []\r\n",
        "segments = []\r\n",
        "labels = []\r\n",
        "max_sequence_length = 64\r\n",
        "for i in range(len(train_nsmc)):\r\n",
        "  text = str(train_nsmc['document'][i])\r\n",
        "  label = train_nsmc['label'][i]\r\n",
        "\r\n",
        "  #token 만드는 부분\r\n",
        "  token = kobert_tokenizer.encode(text, max_length=max_sequence_length, pad_to_max_length=True)\r\n",
        "  for index, value in enumerate(token):\r\n",
        "    if value == 1:\r\n",
        "        token[index] = 0\r\n",
        "\r\n",
        "  #mask 만드는 부분\r\n",
        "  zeros = token.count(0)\r\n",
        "  mask = [1]*(max_sequence_length-zeros) + [0]*zeros\r\n",
        "\r\n",
        "  #segment 만드는 부분\r\n",
        "  segment = [0]*max_sequence_length\r\n",
        "\r\n",
        "\r\n",
        "  #각 리스트에 저장\r\n",
        "  tokens.append(token)\r\n",
        "  masks.append(mask)\r\n",
        "  segments.append(segment)\r\n",
        "  labels.append(label)\r\n",
        "\r\n",
        "# numpy array로 변경\r\n",
        "tokens = np.array(tokens)\r\n",
        "masks = np.array(masks)\r\n",
        "segments = np.array(segments)\r\n",
        "labels = np.array(labels)\r\n",
        "\r\n",
        "train_x = [tokens, masks, segments]\r\n",
        "train_y = labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6jqJGOJoBya",
        "outputId": "0d578f03-0777-4bb1-d663-fd63c7696a7e"
      },
      "source": [
        "train_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[   2, 3093, 1698, ...,    0,    0,    0],\n",
              "        [   2,  517, 7989, ...,    0,    0,    0],\n",
              "        [   2, 1458, 7191, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 3647, 5400, ...,    0,    0,    0],\n",
              "        [   2, 4483, 7467, ...,    0,    0,    0],\n",
              "        [   2, 4958, 3394, ...,    0,    0,    0]]),\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O--wPkKyUvL1",
        "outputId": "f7eab7b0-278d-48de-9840-be10acc54413"
      },
      "source": [
        "test_tokens = []\r\n",
        "test_masks = []\r\n",
        "test_segments = []\r\n",
        "test_labels = []\r\n",
        "max_sequence_length = 64\r\n",
        "for i in range(len(test_nsmc)):\r\n",
        "  text = str(test_nsmc['document'][i])\r\n",
        "  label = test_nsmc['label'][i]\r\n",
        "  #token 만드는 부분\r\n",
        "  token = kobert_tokenizer.encode(text, max_length=max_sequence_length,  pad_to_max_length=True)\r\n",
        "  for index, value in enumerate(token):\r\n",
        "    if value == 1:\r\n",
        "        token[index] = 0\r\n",
        "\r\n",
        "  #mask 만드는 부분\r\n",
        "  zeros = token.count(0)\r\n",
        "  mask = [1]*(max_sequence_length-zeros) + [0]*zeros\r\n",
        "\r\n",
        "  #segment 만드는 부분\r\n",
        "  segment = [0]*max_sequence_length\r\n",
        "\r\n",
        "\r\n",
        "  #각 리스트에 저장\r\n",
        "  test_tokens.append(token)\r\n",
        "  test_masks.append(mask)\r\n",
        "  test_segments.append(segment)\r\n",
        "  test_labels.append(label)\r\n",
        "\r\n",
        "# numpy array로 변경\r\n",
        "test_tokens = np.array(test_tokens)\r\n",
        "test_masks = np.array(test_masks)\r\n",
        "test_segments = np.array(test_segments)\r\n",
        "test_labels = np.array(test_labels)\r\n",
        "\r\n",
        "test_x = [test_tokens, test_masks, test_segments]\r\n",
        "test_y = test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAYo3eU5rUYp",
        "outputId": "6a050419-f4cc-4958-bf27-46cc3be864ca"
      },
      "source": [
        "test_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[   2,  517, 5515, ...,    0,    0,    0],\n",
              "        [   2, 2145, 6844, ...,    0,    0,    0],\n",
              "        [   2, 4297, 6095, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 1212, 5859, ...,    0,    0,    0],\n",
              "        [   2, 4069, 2420, ...,    0,    0,    0],\n",
              "        [   2, 1914, 5760, ...,    0,    0,    0]]),\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAV9bRFjQ8Li",
        "outputId": "4da94052-765e-441f-9766-8f026084e2f9"
      },
      "source": [
        "model = TFBertModel.from_pretrained('monologg/kobert', from_pt=True)\r\n",
        "input_tokens = tf.keras.layers.Input((max_sequence_length, ), dtype=tf.int32, name='input_word_ids')\r\n",
        "input_masks = tf.keras.layers.Input((max_sequence_length, ), dtype=tf.int32, name='input_masks')\r\n",
        "input_segments = tf.keras.layers.Input((max_sequence_length, ), dtype=tf.int32, name='input_segment')\r\n",
        "output = model([input_tokens, input_masks, input_segments])\r\n",
        "output = output[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPXnj1OTE_4"
      },
      "source": [
        "# Rectified Adam 옵티마이저 사용\r\n",
        "import tensorflow_addons as tfa\r\n",
        "# 총 batch size * 4 epoch = 2344 * 4\r\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\r\n",
        "\r\n",
        "#embedding = tf.keras.layers.Embedding(768,128)(output)\r\n",
        "lstm = tf.keras.layers.LSTM(128)(output)\r\n",
        "drop = tf.keras.layers.Dropout(0.5)(lstm)\r\n",
        "dense = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(drop)\r\n",
        "final_model = tf.keras.Model([input_tokens, input_masks, input_segments], dense)\r\n",
        "final_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8qYfH72T5V5",
        "outputId": "d21ed63e-4389-4f93-f5e4-6b918dfc6605"
      },
      "source": [
        "final_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_segment (InputLayer)      [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 92186880    input_word_ids[0][0]             \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 input_segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 128)          459264      tf_bert_model_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 128)          0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            129         dropout_75[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 92,646,273\n",
            "Trainable params: 92,646,273\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLLrWRuT939",
        "outputId": "8c805ded-7081-4e8d-b749-b9cfaa1e7d66"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "\r\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\r\n",
        "#model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_weights_only=True)\r\n",
        "\r\n",
        "final_model.fit(train_x, train_y, epochs=30, shuffle=True,callbacks=[early_stopping], batch_size=64, validation_data=(test_x, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2332/2332 [==============================] - ETA: 0s - loss: 0.5147 - accuracy: 0.7296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2332/2332 [==============================] - 2310s 970ms/step - loss: 0.5146 - accuracy: 0.7297 - val_loss: 0.2851 - val_accuracy: 0.8802\n",
            "Epoch 2/30\n",
            "2332/2332 [==============================] - 2278s 977ms/step - loss: 0.2604 - accuracy: 0.8952 - val_loss: 0.2708 - val_accuracy: 0.8917\n",
            "Epoch 3/30\n",
            "2332/2332 [==============================] - 2278s 977ms/step - loss: 0.1895 - accuracy: 0.9289 - val_loss: 0.2774 - val_accuracy: 0.8941\n",
            "Epoch 4/30\n",
            "2332/2332 [==============================] - 2279s 977ms/step - loss: 0.1241 - accuracy: 0.9572 - val_loss: 0.3274 - val_accuracy: 0.8926\n",
            "Epoch 5/30\n",
            "2332/2332 [==============================] - 2276s 976ms/step - loss: 0.0797 - accuracy: 0.9748 - val_loss: 0.3751 - val_accuracy: 0.8928\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f51407be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "BxVYLUN0X4kX",
        "outputId": "e7b64e20-79a5-4510-bb6e-33a6d7c518c9"
      },
      "source": [
        "# 제출할 데이터\r\n",
        "submit_nsmc = pd.read_table('/content/gdrive/My Drive/Colab Notebooks/naver_sentiment/제출데이터/ko_data.csv', encoding='cp949', delimiter=',')\r\n",
        "submit_nsmc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>정말 많이 울었던 영화입니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>시간 낭비예요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11182</th>\n",
              "      <td>11182</td>\n",
              "      <td>이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11183</th>\n",
              "      <td>11183</td>\n",
              "      <td>심심__ 그냥 한효주 cf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11184</th>\n",
              "      <td>11184</td>\n",
              "      <td>공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11185</th>\n",
              "      <td>11185</td>\n",
              "      <td>오토바이 신은 최고네요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11186</th>\n",
              "      <td>11186</td>\n",
              "      <td>개병헌 쓰면 엉망이 된다ㅋㅋㅋ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11187 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id                                           Sentence\n",
              "0          0                                   정말 많이 울었던 영화입니다.\n",
              "1          1                                           시간 낭비예요.\n",
              "2          2             포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.\n",
              "3          3               지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!\n",
              "4          4                          이걸 영화로 만드는 거야?얼마나 가는지 보자.\n",
              "...      ...                                                ...\n",
              "11182  11182  이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...\n",
              "11183  11183                                     심심__ 그냥 한효주 cf\n",
              "11184  11184  공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...\n",
              "11185  11185                                      오토바이 신은 최고네요.\n",
              "11186  11186                                   개병헌 쓰면 엉망이 된다ㅋㅋㅋ\n",
              "\n",
              "[11187 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9tMAukJkhiy",
        "outputId": "aaeffe27-e601-4be1-d8a6-be08b8112eaf"
      },
      "source": [
        "submit_tokens = []\r\n",
        "submit_masks = []\r\n",
        "submit_segments = []\r\n",
        "max_sequence_length = 64\r\n",
        "for i in range(len(submit_nsmc)):\r\n",
        "  text = str(submit_nsmc['Sentence'][i])\r\n",
        "  #token 만드는 부분\r\n",
        "  token = kobert_tokenizer.encode(text, max_length=max_sequence_length,  pad_to_max_length=True)\r\n",
        "  for index, value in enumerate(token):\r\n",
        "    if value == 1:\r\n",
        "        token[index] = 0\r\n",
        "\r\n",
        "  #mask 만드는 부분\r\n",
        "  zeros = token.count(0)\r\n",
        "  mask = [1]*(max_sequence_length-zeros) + [0]*zeros\r\n",
        "\r\n",
        "  #segment 만드는 부분\r\n",
        "  segment = [0]*max_sequence_length\r\n",
        "\r\n",
        "\r\n",
        "  #각 리스트에 저장\r\n",
        "  submit_tokens.append(token)\r\n",
        "  submit_masks.append(mask)\r\n",
        "  submit_segments.append(segment)\r\n",
        "\r\n",
        "# numpy array로 변경\r\n",
        "submit_tokens = np.array(submit_tokens)\r\n",
        "submit_masks = np.array(submit_masks)\r\n",
        "submit_segments = np.array(submit_segments)\r\n",
        "\r\n",
        "submit_x = [submit_tokens, submit_masks, submit_segments]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C50CP__pl5YJ",
        "outputId": "616c71c2-2753-4d47-8b2a-47773e712053"
      },
      "source": [
        "preds = final_model.predict(submit_x)\r\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X986vatWzejg"
      },
      "source": [
        "float(preds[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AebbqehqymVN"
      },
      "source": [
        "submit_nsmc['Predicted']=0\r\n",
        "for i in range(len(preds)):\r\n",
        "  score = float(preds[i])\r\n",
        "  if score > 0.5:\r\n",
        "    score = 1\r\n",
        "  else :\r\n",
        "    score = 0\r\n",
        "  submit_nsmc['Predicted'][i]=score\r\n",
        "\r\n",
        "submit_nsmc.drop(['Sentence'], axis='columns', inplace=True)\r\n",
        "submit_nsmc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFNdll9I0Rr-"
      },
      "source": [
        "submit_nsmc.to_csv('/content/gdrive/My Drive/Colab Notebooks/naver_sentiment/제출데이터/nsmc 데이터 결과2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
